[文章翻译自](https://ngrok.com/blog/prompt-caching/)
在我撰写这篇文章时，对于 OpenAI 和 Anthropic 的 API 而言，缓存输入token的价格比常规输入token便宜 10 倍（以美元计）。

Anthropic 甚至声称提示缓存能对于长提示词能降低延迟“高达85%”，我自己的测试页证明了这点，对于足够长的提示词来说。我向 Anthropic 和 OpenAI 发送了数百个请求，发现对于每个输入被标记为缓存的请求，首次输出token的时间延迟大幅减少。

# LLM 是如何工作的？
## LLM 架构
从本质上讲，LLM 是一个巨大的数学函数，它们接收一系列的数字作为输入，并产生一个数字作为输出，LLM内部包含一个庞大的运算图，其中数十亿精心排列的运算，将这些输入数字转为输出数字。
这个庞大的运算图大致可分为四个部分：

* 分词器（Tokenizer）
* 嵌入器 (Embedding)
* 转换器 (Transformer)
  * 注意力机制 (Attention)
  * 反馈机制 （Feedforward）
* 输出器 (Output)

每个图标中的节点可以被认为是一个能够接收输入和产生输出的函数，输入会循环的送入到LLM，直到一个特殊的输出值指示他停止为止。一下是他伪代码的示例：
````
prompt = "What is the meaning of life?"

tokens = tokenizer(prompt)

while(true):
    embeddings = embed(tokens)
    for [attention,feedforward] of transformers:
        embeddings = attention(embeddings)
        embeddings = feedforward(embeddings)
    output_token = output(embeddings)
    if out_token == END_TOKEN:
        break
    tokens.push(output_token)

print(decode(tokens))

````
>LLM 其实比大家想象的要简单
>
>虽然以上的内容已经非常简化了，但令我惊讶的是LLM的代码行数还是很少。
>
>[Sebastian Raschka](https://magazine.sebastianraschka.com) 使用 PyTorch 创建了开源模型的独立重实现版本，他还创作了大量其他一流的教育资料，如果您喜欢这篇文章，您一定会喜欢。例如，目前领先的开源模型之一 Olmo 3，代码量只有几百行。


## 分词器（Tokenizer）
在 LLM 对你的提示词做任何操作之前，它需要转换提示词到一个他能处理的表现形式，而不是原始的提示词。这是一个两步过程，由分词器和嵌入共同参与。直到我们讲到嵌入部分，才会明白为什么这一切都是必要的，所以请耐心听我解释一下分词器是如何工作的。

分词器会接收你的提示，将其分割成小块，并为每个唯一的小块分配一个称为“词元(token)”的整数 ID。例如，以下是 GPT-5 对提示“Check out ngrok.ai”进行分词的方式：

![](/blogs/Prompt-caching/b99d1f80b11cb870.png)

这个提示词已经被分成一个数组 ["Check", " out", " ng", "rok", ".ai"] 并且转换为token[4383, 842, 1657, 17690, 75584]，想同的提示词会得到想同的token结果，token是大小写敏感的，这是因为大写字母可以告诉你一些关于单词的信息，例如，首字母大写的“Will”比首字母小写的“will”更有可能是一个名字。

>为什么不直接按空格或字符分割呢？
>
>这是一个出乎意料的大问题，详细讨论的话，这篇文章的篇幅可能要翻倍。简短但令人失望的答案是：这是一种权衡。如果你想深入了解，[Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE) 有一个很棒的视频，他在视频中从零开始构建了一个分词器。对于当前来说，只需知道分词是将文本转换为数字即可。

Token 是LLM的输入和输出的基本单位，当你问chatgpt 一个问题，LLM 的每个迭代完成后，响应是以流式的token反个你的。提供者这么做的原因是全部的响应可能要耗时数十秒。但是发给你每个准备好的token让整个过程更具互动性

输入提示词，✨人工智能运行✨，输出词，重复此过程。这个过程称为“推理”。请注意，每次迭代之前，输出词都会附加到输入提示词之后。逻辑逻辑模型（LLM）需要完整的上下文信息才能生成正确的答案。如果我们只输入提示词，它会不断尝试生成答案的第一个词。如果我们只输入答案，它会立即忘记问题。每次迭代都需要将完整的提示词和答案输入到 LLM 中。
>199999 \<END\> 标记是什么意思？
>
>推理过程总会在某个时刻停止。LLM（逻辑推理模型）可以输出各种“特殊”标记，其中一种标记表示响应的结束。在 GPT-5 分词器中，这个标记是 199999。这是 LLM 终止的众多方式之一。您可以通过 API 指定要生成的最大标记数量，提供商可能还会制定其他与安全相关的规则来规定何时停止。
>
>此外，还有一些特殊标记用于表示对话消息的开始和结束，聊天模型（例如 ChatGPT 和 Claude）就是通过这些标记来判断一条消息何时结束、另一条消息何时开始的。

关于分词器的最后一点：它们有很多种类！ChatGPT用的分词器和Clade用的分词器不是一种，甚至OpenAI自己的不同的模型的分词器也是不同的，每个分词器有他自己的分词成token 的规则，如果你想了解不同的分词器如何分割文本的，可以查看[titokenizer](https://tiktokenizer.vercel.app/)

## 嵌入（Embedding）
现在我们的 tokens 从分词器到了进入嵌入阶段，要理解embedding，首先理解模型的目标是什么。

当人们用代码解决问题的时候，我们写一个函数来接收输入值产生输出值，例如华氏度转摄氏度：
````
function fahrenheitToCelsius(fahrenheit) {
	return ((fahrenheit - 32) * 5) / 9
}
````

我们可以扔任何数字给这个方法并得到正确的答案，但是如果我们遇到一个我们不知道公式的问题改怎么办？如果我们只有下面这张神秘的输入输出表格呢？

| Input      | Output |
| ----------- | ----------- |
| 21     | 73       |
| 2   | 3       |
| 10  | 29       |
| 206  | 1277       |

我并不指望你能认出这里的功能，不过我需要告诉你ChatGPT会直接弄明白，如果你把这个表格粘贴到app中。

当我们知道每个输入的预期输出，而不是产生它的函数，我们可以训练一个模型去学习这个方法，我们实现它通过给出模型一个画布--巨大的数据运算图-并修改这个图知道模型收敛为正确的函数。每次这个运算图更新，我们运行输入数据，以查看它与正确输出的接近程度。我们一直这样做，直到我们满意它足够的收敛为止，这就是训练的意义。

事实证明，当训练的模型输出正确的文本时候，能够识别两个相似的句子是很有帮助的。但是相似之处提现在哪里呢？它们可能是同样的的悲伤，或者开心，或者发人深省，也可能是想同长度，节奏，语调，语言，词汇，结构，有一个我们能用来描述两句话相似的大量的维度，并且语句相似再某些维度可能很相似但是其他维度则不是。

Tokens 没有维度，它们只是普通的整数，但是嵌入，却有很多维度。

一个嵌入式一个长度为n的数组，代表在n维空间的位置。如果n是3，有一个嵌入可能是\[10,4,2\] 代表着位置信息：x=10，y=4，z=3 在一个三维空间内。当LLM 被训练时，每个token被分配一个随机的起始位置，并且训练过程会轻轻推动所有的token，直到找到能够产生最佳输出的排列方式

嵌入阶段首先查找每个token的嵌入向量，伪代码如下：
````
# 在训练的时候创建，在推理的时候从不改变
const EMBEDDINGS =[...]

def embed(tokens)
    return tokens.map(token->{return EMBEDDINGS[token]})

````
所以我们取词元（一个整数数组），并将其转换为嵌入数组。这是一个数组的数组，或者说是一个“矩阵”。在下面的词元和嵌入之间切换，可以看到我脑海中对这个过程的理解。

tokens \[75, 305, 284, 887\] 被转换为一个3维的矩阵嵌入

嵌入的矩阵维度越多，就需要比较更多维度的句子。我们已经讨论了三维，但是当前模型的嵌入维度已经到了数千，最大的一个甚至超过了10000。

为了展现更多维度的价值，以下我有八组颜色的形状，开始于一维空间，他们在一条直线上，并且看起来杂乱无章难以理解，但是当你加入更多的维度，就变得清晰他们是八个不同的清楚的区域，这里我只能只能可视化的三维的情况，你可以用你的想象力去想象在成千上万维的情况下你能做什么

嵌入阶段还有最后一步。在获取token嵌入向量后，它会将token在提示符中的位置编码到嵌入向量中。我没有深入研究其工作原理，但如果没有这一步，LLM 就无法确定提示符中词元的顺序。
为了更新我们之前的伪代码，假设存在一个名为 encodePosition 的函数。它接受向量嵌入和一个位置值作为参数，并返回包含位置编码的新向量嵌入。
````
const EMBEDDINGS =[...]

def embed(tokens)
    return tokens.map((token,i)->{
        const embeddings = EMBEDDINGS[token]
        return encodePosition(embeddings, i);
        }
    )
````

总而言之，嵌入是n维空间中的点，你可以将其理解为它们代表文本的语义含义，在训练过程中，每个token都会在这个空间中移动，使其与其他相似的token尽可能接近。维度越多，LLM对每个token的表示就越复杂、越细致。

我们在分词器和嵌入阶段所做的所有工作，都是为了将​​文本转换成LLM可以处理的格式。现在让我们来看看在转换器阶段，这项工作具体是如何进行的。

## 转换器 Transformer

Transformer 阶段的核心在于将嵌入向量作为输入，并在其 n 维空间中移动它们。它通过两种方式实现这一点，我们只关注第一种：注意力机制。本文暂不讨论“前馈”或输出阶段（👀）。

注意力机制的作用是帮助语言学习模型（LLM）理解提示中每个token之间的关系，它允许token在n维空间中相互影响彼此的位置。其实现方式是将提示中所有token的嵌入向量进行加权组合。输入是整个提示的嵌入向量，输出是所有输入嵌入向量的加权组合，得到一个新的嵌入向量。

例如，如果我们有提示“Mary had a little”，并且得到了四个token：Mary、had、a 和 little，那么注意力机制可能会决定，为了生成下一个token，我们应该使用：

63% of Mary's embeddings
16% of had's embeddings
12% of a's embeddings
9% of little's embeddings

它会将所有词嵌入按权重缩放并求和。这就是 LLM 如何知道应该对提示中的每个token给予多少关注，或者说“重视”多少。

这是目前为止整个过程中最复杂、最抽象的部分。我将首先用伪代码来展示，然后我们再来看看词嵌入是如何经过这一过程的。我原本想尽量减少这部分的数学运算，但这里很难完全避免一些数学知识。我相信你能做到。

注意力机制中的大部分计算都是矩阵乘法。关于矩阵乘法，你只需要知道输出矩阵的形状由输入矩阵的形状决定。输出矩阵的行数始终与第一个输入矩阵相同，列数始终与第二个输入矩阵相同。


基于以上考虑，以下是一个简化的注意力机制如何计算分配给每个token的权重。在下面的代码中，我使用 * 表示矩阵乘法。

````
// 与伪代码中的 EMBEDINGS 类似
// 前面提到的 WQ 和 WK 在训练过程中学习
// 并且在推理过程中保持不变。
// 这两个都是 n*n 矩阵，其中 n 是
// 嵌入维度的数量。在我们的示例中
// n = 3。

const WQ = [[...], [...], [...]];
const WK = [[...], [...], [...]];

// 输入嵌入如下所示：
// [
// [-0.1, 0.1, -0.3], // Mary
// [1.0, -0.5, -0.6], // had
// [0.0, 0.8, 0.6], // a
// [0.5, -0.7, 1.0] // little
// ]

function attentionWeights(embeddings) {

    const Q = embeddings * WQ;

    const K = embeddings * WK;

    const scores = Q * transpose(K);

    const masked = mask(scores);

    return softmax(masked);

}
````

让我们来看看嵌入是如何在这个函数中传递的。

>等等，WQ 和 WK 变量是什么？
>
>还记得我之前说过，每个词元的嵌入向量都会被随机分配一个位置，然后训练过程会不断微>调这些位置，直到模型收敛到一个合适的排列吗？
>
>WQ 和 WK 与之类似。它们是 n×n 的矩阵，其中 n 是嵌入向量的维度，在训练开始时会被赋予随机值。然后在训练过程中，它们也会被微调，以帮助模型收敛到一个合适的解。
>
>任何在训练过程中被微调的元素都被称为“模型参数”。嵌入向量以及 WQ 和 WK 矩阵中的每个浮点数都是一个参数。当你听到一个模型被描述为“拥有 1750 亿个参数”时，指的就是这些参数。
>
>至于 WQ 和 WK 究竟是什么，我们目前还不太清楚。随着模型的收敛，它们最终会呈现出某种嵌入变换，这种变换有助于模型产生良好的输出。它们可能在做任何事情，并将其中的内容解释为一个开放且活跃的研究领域。

为了得到 Q 和 K，我们分别将嵌入向量乘以 WQ 和 WK。WQ 和 WK 的行数和列数始终等于嵌入向量的维度数，在本例中为 3。这里我为 WQ 和 WK 选取了随机值，为了便于阅读，数值四舍五入到小数点后两位。

![](/blogs/Prompt-caching/c3f26753eebba919.png)


得到的 Q 矩阵有 4 行 3 列。4 行是因为嵌入矩阵有 4 行（每行代表一个词元），3 列是因为 WQ 有 3 列（每列代表一个嵌入维度）。


K 的计算方法完全相同，只是用 WK 代替了 WQ。


Q 和 K 都是输入嵌入到新的 n 维空间中的“投影”。它们并非原始嵌入，而是由原始嵌入导出。

然后，我们将 Q 和 K 相乘。我们对 K 进行“转置”，即沿对角线翻转，使得到的矩阵为方阵，其行数和列数分别等于输入提示中的词元数。


这些分数代表每个词元对下一个生成的词元的重要性。左上角的数字 -0.08 表示“Mary”对“had”的重要性。再往下一行，-0.10 表示“Mary”对“a”的重要性。矩阵运算之后，我会展示一个可视化图表。接下来的所有操作都是将这些分数转换为权重，以便我们混合词嵌入。

这个分数矩阵的第一个问题是它允许未来的词元影响过去的词元。在第一行，我们只知道“Mary”这个词，所以它应该是唯一对“had”的生成有贡献的词。第二行也是如此，我们知道“Mary”和“had”，所以只有这两个词应该对“a”的生成有贡献，依此类推。

为了解决这个问题，我们对矩阵应用一个三角掩码，将未来的词元置零。但是，我们不是将它们置零，而是将它们设置为负无穷大。我稍后会解释原因。


第二个问题是这些分数是任意的。如果它们是一个每行总和为 1 的分布，对我们来说会更有用。这正是 softmax 函数的作用。softmax 的具体工作原理并不重要，它比简单地将每个数字除以该行的总和略微复杂一些，但结果是一样的：每行的总和为 1，且每个数字都在 0 到 1 之间。

为了解释负无穷大，这里给出softmax函数的代码实现：
````

function softmax(matrix) {
	return matrix.map(row => {
		const exps = row.map(x => Math.exp(x));
		const sumExps = exps.reduce((a, b) => a + b, 0);
		return exps.map(exp => exp / sumExps);
	});
}

````

它并非直接将所有数字相加，然后再用每个数字除以总和。相反，它首先计算每个数字的指数函数，即 e^x。如果我们用零代替负无穷大，那么 `Math.exp(0) === 1`，这意味着零仍然会产生权重。`Math.exp(-∞)` 等于 0，这正是我们想要的。

下面的表格展示了提示词“Mary had a little”的注意力权重示例。您可以将鼠标悬停在表格单元格上或点击单元格，查看每个词元的贡献值。这些权重与上面的计算结果不符，因为它们取自[Transformer Explained](https://poloclub.github.io/transformer-explainer/)网站上运行的GPT-2版本。所以这些权重来自一个真实的（尽管是旧版本）模型。

第一行只有“Mary”，所以“Mary”对“had”的贡献率为100%。第二行中，“Mary”对“a”的生成贡献率为79%，而“had”贡献率为21%，以此类推。LLM认为这句话中最重要的词是“Mary”，这并不奇怪，因为“Mary”在每一行的权重都最高。如果我让你完成句子“Jessica had a little”，你不太可能选择“lamb”。


剩下的就是对词嵌入进行混合，这比生成权重要简单得多。

与之前类似，我们有一个在训练时确定的WV矩阵。我们利用这个矩阵从词嵌入中得到V矩阵。


>为什么不直接混合词嵌入呢？
>
>当我们推导出 Q 和 K，然后将它们相乘得到注意力权重时，我们完全是在基于词元之间的相关性进行操作。词嵌入编码了词元的各种语义信息，一个维度可以代表“颜色”，另一个维度可以代表“大小”，还有一个维度可以代表“粗鲁程度”，等等。权重则利用相似性来确定相关性。
>
>WV 允许模型决定保留哪些维度。在句子“Mary had a little”中，关于 Mary 的重要信息是她的名字。模型可能还学习到了很多关于血腥玛丽鸡尾酒或苏格兰女王玛丽的信息。这些信息与这首童谣无关，保留它们会引入噪声。因此，WV 允许模型在混合词嵌入之前过滤掉无关特征。

然后我们将该 V 乘以我们生成的权重，输出结果就是一组新的嵌入：


注意力机制的最终输出是该输出矩阵的最后一行。之前所有词元的上下文信息都通过注意力过程混合到了最后一行中，但前提是之前的每一行都需要重新计算。

总而言之，输入词嵌入，输出一个新的词嵌入。注意力机制执行了大量复杂的数学运算，根据训练过程中学习到的 WQ、WK 和 WV 矩阵，按照词元的重要性比例将它们混合在一起。正是这种机制使得 LLM 能够了解其上下文窗口中哪些词元是重要的，以及为什么重要。


>注意力机制远不止于此

>我在这里展示的是一个简化版的注意力机制（我知道，这确实很简化）。实际上，注意力机制远不止于此。如果您有兴趣深入了解，我推荐观看 (3blue1brown)[https://www.youtube.com/watch?v=eMlx5fFNoYc] 关于注意力机制的视频。